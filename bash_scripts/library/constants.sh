#!/bin/bash

################################################################################
# constants.sh - Shared constants and configuration
#
# DESCRIPTION:
# Defines project-wide constants used across multiple scripts including:
# - Default directory paths (relative paths for data, output, logs)
# - Standard file naming conventions for all pipeline stages
# - Log formatting configuration
# - Validation rules and patterns
# - Python environment settings
#
# USAGE:
# Source this file in your script:
#   source "${LIBRARY_SCRIPTS_DIRECTORY_PATH}/constants.sh"
#
# Access constants:
#   echo "$DEFAULT_RAW_DATA_DIR"
#   output_file="${DEFAULT_CSV_SINGLE_VALUED}"
#
# NOTES:
# - All constants are declared as readonly to prevent accidental modification
# - Paths are relative to the project root (bash_scripts/ directory)
# - File naming conventions follow the project standards
#
################################################################################

# MULTIPLE SOURCING GUARD

# Prevent multiple sourcing of this script by returning if already included
[[ -n "${CONSTANTS_SH_INCLUDED}" ]] && return
CONSTANTS_SH_INCLUDED=1

# =============================================================================
# DIRECTORY PATHS
# =============================================================================

# Default data directories (relative to bash_scripts/)
# These paths assume scripts are run from bash_scripts/ directory
readonly DEFAULT_RAW_DATA_DIR="../data_files/raw"
readonly DEFAULT_PROCESSED_DATA_DIR="../data_files/processed"
readonly DEFAULT_OUTPUT_DIR="../output"
readonly DEFAULT_PLOTS_DIR="../output/plots"
readonly DEFAULT_TABLES_DIR="../output/tables"
readonly DEFAULT_LOGS_DIR="../logs"

# Python scripts directory
readonly DEFAULT_PYTHON_SCRIPTS_DIR="../core/src"

# =============================================================================
# PARSING STAGE - File Naming Conventions (Stage 1)
# =============================================================================

# Output files from parse_log_files.py
readonly PARSING_CSV_SINGLE_VALUED="single_valued_parameters.csv"
readonly PARSING_HDF5_MULTIVALUED="multivalued_parameters.h5"

# Output files from parse_correlator_files.py
readonly PARSING_HDF5_CORRELATORS="correlators_raw_data.h5"

# =============================================================================
# PROCESSING STAGE - File Naming Conventions (Stage 2)
# =============================================================================

# Output from process_raw_parameters.py
readonly PROCESSING_CSV_PROCESSED="processed_parameter_values.csv"

# Output from apply_jackknife_analysis.py
readonly PROCESSING_HDF5_JACKKNIFE="correlators_jackknife_analysis.h5"

# =============================================================================
# ANALYSIS STAGE - File Naming Conventions (Stage 3)
# =============================================================================

# Stage 3.1 - Correlator Calculations
readonly ANALYSIS_HDF5_PCAC_MASS="PCAC_mass_analysis.h5"
readonly ANALYSIS_HDF5_PION_MASS="pion_effective_mass_analysis.h5"

# Stage 3.2 - Plateau Extraction
readonly ANALYSIS_CSV_PCAC_PLATEAU="plateau_PCAC_mass_estimates.csv"
readonly ANALYSIS_HDF5_PCAC_PLATEAU="plateau_PCAC_mass_estimates.h5"
readonly ANALYSIS_CSV_PION_PLATEAU="plateau_pion_mass_estimates.csv"
readonly ANALYSIS_HDF5_PION_PLATEAU="plateau_pion_mass_estimates.h5"

# Stage 3.3 - Critical Mass Extrapolation
readonly ANALYSIS_CSV_CRITICAL_PCAC="critical_bare_mass_from_pcac.csv"
readonly ANALYSIS_CSV_CRITICAL_PION="critical_bare_mass_from_pion.csv"

# Stage 3.4 - Cost Extrapolation
readonly ANALYSIS_CSV_COST_PCAC="cost_extrapolation_from_pcac.csv"
readonly ANALYSIS_CSV_COST_PION="cost_extrapolation_from_pion.csv"

# =============================================================================
# LOG FILE NAMING
# =============================================================================

# Log file suffixes
readonly LOG_FILE_SUFFIX=".log"
readonly PYTHON_LOG_SUFFIX="_python_script.log"
readonly PIPELINE_LOG_SUFFIX="_pipeline.log"

# =============================================================================
# SUMMARY FILE NAMING
# =============================================================================

# HDF5 structure tree files (generated by h5glance)
readonly HDF5_TREE_SUFFIX="_tree.txt"

# CSV summary report files (generated by inspect_csv_file.py)
readonly CSV_SUMMARY_SUFFIX="_uniqueness_report.txt"

# =============================================================================
# TIMESTAMP MANAGEMENT
# =============================================================================

# Timestamp file suffix for caching
readonly TIMESTAMP_FILE_SUFFIX=".timestamp"

# Timestamp file directory name
readonly TIMESTAMP_DIR_NAME=".timestamps"

# =============================================================================
# LOG FORMATTING
# =============================================================================

# Date format for log timestamps
readonly LOG_DATE_FORMAT="%Y-%m-%d %H:%M:%S"

# Message wrap width for log entries
readonly LOG_WRAP_WIDTH=80

# Log level names (for reference)
readonly LOG_LEVEL_DEBUG="DEBUG"
readonly LOG_LEVEL_INFO="INFO"
readonly LOG_LEVEL_WARNING="WARNING"
readonly LOG_LEVEL_ERROR="ERROR"
readonly LOG_LEVEL_CRITICAL="CRITICAL"

# =============================================================================
# FILE VALIDATION
# =============================================================================

# Valid file extensions for QPB data files
readonly -a VALID_RAW_FILE_EXTENSIONS=("txt" "dat" "err")

# Valid processed file extensions
readonly -a VALID_PROCESSED_FILE_EXTENSIONS=("csv" "h5" "hdf5" "log")

# Valid output file extensions
readonly -a VALID_OUTPUT_FILE_EXTENSIONS=("png" "jpg" "pdf" "svg" "html" "tex")

# =============================================================================
# PYTHON ENVIRONMENT
# =============================================================================

# Python command (can be overridden by environment variable)
readonly PYTHON_CMD="${PYTHON_CMD:-python}"

# Required Python packages for the project
readonly -a REQUIRED_PYTHON_PACKAGES=(
    "numpy"
    "pandas"
    "h5py"
    "click"
    "matplotlib"
    "scipy"
)

# Optional Python packages (for enhanced functionality)
readonly -a OPTIONAL_PYTHON_PACKAGES=(
    "gvar"
    "lsqfit"
)

# =============================================================================
# EXTERNAL TOOLS
# =============================================================================

# Required external command-line tools
readonly -a REQUIRED_TOOLS=(
    "python"
    "realpath"
    "basename"
    "dirname"
)

# Optional external tools (for enhanced functionality)
readonly -a OPTIONAL_TOOLS=(
    "h5glance"
    "h5dump"
    "h5ls"
)

# =============================================================================
# PIPELINE STAGE NAMES
# =============================================================================

# Stage identifiers for logging and reporting
readonly STAGE_1_NAME="Parsing"
readonly STAGE_2_NAME="Processing"
readonly STAGE_3_1_NAME="Correlator Calculations"
readonly STAGE_3_2_NAME="Plateau Extraction"
readonly STAGE_3_3_NAME="Critical Mass Extrapolation"
readonly STAGE_3_4_NAME="Cost Extrapolation"

# =============================================================================
# ANALYSIS TYPES
# =============================================================================

# Analysis type identifiers
readonly ANALYSIS_TYPE_PCAC="pcac"
readonly ANALYSIS_TYPE_PION="pion"
readonly ANALYSIS_TYPE_PCAC_MASS="pcac_mass"
readonly ANALYSIS_TYPE_PION_MASS="pion_mass"

# =============================================================================
# SCRIPT EXIT CODES
# =============================================================================

# Standard exit codes for consistent error reporting
readonly EXIT_SUCCESS=0
readonly EXIT_GENERAL_ERROR=1
readonly EXIT_INVALID_ARGS=2
readonly EXIT_FILE_NOT_FOUND=3
readonly EXIT_PERMISSION_DENIED=4
readonly EXIT_PYTHON_SCRIPT_FAILED=5
readonly EXIT_VALIDATION_FAILED=6
readonly EXIT_INTERRUPTED=130  # SIGINT (Ctrl+C)

# =============================================================================
# WORKFLOW SCRIPT NAMES
# =============================================================================

# Main workflow scripts (for reference and validation)
readonly WORKFLOW_PARSING="run_parsing_pipeline.sh"
readonly WORKFLOW_PROCESSING="run_processing_pipeline.sh"
readonly WORKFLOW_CORRELATOR_ANALYSIS="run_correlator_analysis_pipeline.sh"
readonly WORKFLOW_PLATEAU_ANALYSIS="run_plateau_analysis_pipeline.sh"
readonly WORKFLOW_CRITICAL_MASS="run_critical_mass_pipeline.sh"
readonly WORKFLOW_COST_ANALYSIS="run_cost_analysis_pipeline.sh"
readonly WORKFLOW_FULL_PIPELINE="run_full_pipeline.sh"

# =============================================================================
# DISPLAY FORMATTING
# =============================================================================

# Progress indicators
readonly PROGRESS_SUCCESS="✓"
readonly PROGRESS_SKIPPED="○"
readonly PROGRESS_FAILED="✗"
readonly PROGRESS_WARNING="⚠"
readonly PROGRESS_INFO="ℹ"

# Section separators
readonly SEPARATOR_HEADER="==================================================================="
readonly SEPARATOR_SECTION="-------------------------------------------------------------------"

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

function get_parsing_output_csv() {
    # Get full path to parsing stage CSV output
    #
    # Arguments:
    #   $1 - output_directory : Output directory path
    #
    # Returns:
    #   Prints full path to stdout
    #
    # Example:
    #   csv_path=$(get_parsing_output_csv "$output_dir")
    
    echo "${1}/${PARSING_CSV_SINGLE_VALUED}"
}

function get_parsing_output_hdf5_params() {
    # Get full path to parsing stage HDF5 parameters output
    #
    # Arguments:
    #   $1 - output_directory : Output directory path
    #
    # Returns:
    #   Prints full path to stdout
    #
    # Example:
    #   hdf5_path=$(get_parsing_output_hdf5_params "$output_dir")
    
    echo "${1}/${PARSING_HDF5_MULTIVALUED}"
}

function get_parsing_output_hdf5_correlators() {
    # Get full path to parsing stage HDF5 correlators output
    #
    # Arguments:
    #   $1 - output_directory : Output directory path
    #
    # Returns:
    #   Prints full path to stdout
    #
    # Example:
    #   hdf5_path=$(get_parsing_output_hdf5_correlators "$output_dir")
    
    echo "${1}/${PARSING_HDF5_CORRELATORS}"
}

function get_processing_output_csv() {
    # Get full path to processing stage CSV output
    #
    # Arguments:
    #   $1 - output_directory : Output directory path
    #
    # Returns:
    #   Prints full path to stdout
    #
    # Example:
    #   csv_path=$(get_processing_output_csv "$output_dir")
    
    echo "${1}/${PROCESSING_CSV_PROCESSED}"
}

function get_processing_output_hdf5() {
    # Get full path to processing stage HDF5 jackknife output
    #
    # Arguments:
    #   $1 - output_directory : Output directory path
    #
    # Returns:
    #   Prints full path to stdout
    #
    # Example:
    #   hdf5_path=$(get_processing_output_hdf5 "$output_dir")
    
    echo "${1}/${PROCESSING_HDF5_JACKKNIFE}"
}

function get_timestamp_directory() {
    # Get full path to timestamp directory for caching
    #
    # Arguments:
    #   $1 - base_directory : Base directory (usually processed data dir)
    #
    # Returns:
    #   Prints full path to stdout
    #
    # Example:
    #   timestamp_dir=$(get_timestamp_directory "$processed_dir")
    
    echo "${1}/${TIMESTAMP_DIR_NAME}"
}

# =============================================================================
# END OF FILE
# =============================================================================