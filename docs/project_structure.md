# Project Structure Documentation

This document provides a comprehensive overview of the
`qpb_data_analysis` project structure, explaining the purpose and
organization of each directory.

## Overview

The project follows a modular architecture that separates data
processing, analysis, and presentation into distinct components:

```
qpb_data_analysis/
├── core/                   # Main Python package
├── bash_scripts/          # Processing pipeline automation
├── data_files/            # Data storage (raw and processed)
├── output/                # Analysis results
├── notebooks/             # Interactive analysis environment
├── examples/              # Usage demonstrations
└── docs/                  # Documentation
```

---

## Directory Details

### `core/` - Main Python Package

The heart of the project containing all Python code for data processing
and analysis.

#### `core/library/`
**Purpose**: Generic, reusable utility functions and classes that can be
used across different parts of the project.

**Contains**:
- File I/O utilities
- Data structure helpers
- Mathematical functions
- General-purpose algorithms
- Configuration management

**Key Characteristic**: Domain-agnostic code that could potentially be
reused in other projects.

#### `core/src/`
**Purpose**: Domain-specific analysis scripts and modules for the main
data analysis pipeline.

**Contains**:
- Data parsing scripts (log files, correlator data)
- Analysis algorithms specific to qpb data
- Plotting and visualization modules
- Data processing workflows

**Key Characteristic**: Code specifically tailored to qpb data analysis
tasks.

**Relationship with bash_scripts**: Scripts in `core/src/` are invoked
by BASH automation scripts in `bash_scripts/` as part of the processing
pipeline.

#### `core/tests/`
**Purpose**: Ensure code quality and correctness through automated
testing.

**Contains**:
- Unit tests for individual functions and classes
- Integration tests for workflow components
- Mock data for testing (in `mock_data/` subdirectory)
- Test utilities and fixtures

**Best Practices**:
- Mirror the structure of `core/library/` and `core/src/`
- Name test files with `test_` prefix
- Run regularly during development

---

### `bash_scripts/` - Pipeline Automation

**Purpose**: Automate the data processing and analysis workflow using
BASH scripts.

**Main Responsibilities**:
1. **Pre-processing**: Validate and prepare raw data files
2. **Processing**: Invoke Python scripts from `core/src/` to transform
   raw data
3. **Analysis**: Execute analysis workflows on processed data
4. **Orchestration**: Coordinate multi-step pipelines

**Typical Workflow**:
```
Raw Data → Validation → Processing → Analysis → Output
```

**Key Features**:
- Directory traversal and batch processing
- Timestamp-based caching to avoid redundant processing
- Logging and error handling
- Environment variable management

**Subdirectories** (suggested organization):
```
bash_scripts/
├── workflows/          # Main pipeline scripts
├── checks/             # Validation and inspection scripts
├── utils/              # Utility scripts (backup, cleanup, etc.)
└── library/            # Shared BASH functions
```

---

### `data_files/` - Data Storage

Central repository for all input and output data files.

#### `data_files/raw/`
**Purpose**: Store original, unmodified data files exactly as generated
by qpb programs.

**Structure**:
```
raw/
└── <experiment_name>/
    ├── file1.txt       # Log files
    ├── file2.dat       # Correlator data
    └── file3.err       # Error logs
```

**Allowed File Extensions**:
- `.txt` - qpb program execution logs
- `.dat` - Raw correlator data from mesons program
- `.err` - Error logs

**Naming Convention**: Use descriptive experiment names, e.g.:
- `Chebyshev_several_config_varying_N`
- `KL_several_vectors_varying_configs_and_n`

**Golden Rule**: NEVER modify files in this directory. Keep raw data
pristine.

#### `data_files/processed/`
**Purpose**: Store data files after processing by the analysis pipeline.

**File Types**:
- `.csv` - Tabular data for easy inspection and further analysis
- `.h5` (HDF5) - Efficient storage for large numerical datasets
- `.log` - Analysis logs documenting processing steps
- `.md` - Markdown notes and metadata

**Structure**: Mirrors `raw/` directory organization:
```
processed/
└── <experiment_name>/
    ├── experiment.csv
    ├── experiment.h5
    ├── experiment.log
    └── notes.md
```

**Key Characteristic**: These files are generated by scripts and can be
regenerated from raw data.

---

### `output/` - Analysis Results

**Purpose**: Store final analysis outputs intended for presentation or
publication.

#### `output/plots/`
**Purpose**: Store generated visualizations and figures.

**File Types**: `.png`, `.jpg`, `.pdf`, `.svg`

**Organization**: Can be organized by experiment or analysis type as
needed.

#### `output/tables/`
**Purpose**: Store formatted tables for reports and publications.

**File Types**: 
- `.html` - Web-friendly tables
- `.tex` / `.latex` - LaTeX tables for academic papers
- Formatted `.csv` with presentation styling

**Note**: Distinguished from `data_files/processed/*.csv` which are
intermediate data products rather than presentation-ready tables.

---

### `notebooks/` - Interactive Analysis

**Purpose**: Provide Jupyter notebooks for interactive exploration and
analysis of processed data.

**Use Cases**:
- Exploratory data analysis (EDA)
- Custom visualizations beyond the main pipeline
- Prototyping new analysis methods
- Creating presentation-ready figures
- Interactive parameter studies

**Suggested Organization**:
```
notebooks/
├── exploratory/        # EDA and data exploration
├── visualization/      # Custom plotting notebooks
├── templates/          # Template notebooks for common tasks
├── README.md          # Guide for notebook usage
└── environment.yml    # Notebook-specific dependencies (optional)
```

**Input**: Primarily uses `.csv` and `.h5` files from
`data_files/processed/`

**Output**: Can save results to `output/` directories as needed

**Best Practice**: Use notebooks for exploration and iteration, then
migrate stable analyses to production scripts in `core/src/`.

---

### `examples/` - Usage Demonstrations

**Purpose**: Provide working examples to help users understand how to
use the project.

**Contains**:
- Example scripts demonstrating common workflows
- Sample data for running examples
- Tutorial notebooks
- Usage patterns and best practices

**Suggested Organization**:
```
examples/
├── example_data/       # Small sample datasets
├── basic_workflow.py   # Simple end-to-end example
├── advanced_usage.py   # Complex use cases
└── README.md          # Guide to examples
```

**Note**: This directory is for future development as the project
matures.

---

### `docs/` - Documentation

**Purpose**: Comprehensive project documentation for users and
developers.

**Current Structure**:
```
docs/
├── PROJECT_STRUCTURE.md    # This file - architecture overview
├── INSTALL.md             # Installation instructions
├── USAGE.md               # User guide
├── API.md                 # API reference (optional)
├── organizing_data_files.md  # Data organization guidelines
└── project_plan.txt       # Original project plan (archived)
```

**Best Practices**:
- Keep documentation up to date with code changes
- Use Markdown for easy viewing on GitHub
- Include code examples where appropriate
- Maintain both user-facing and developer documentation

---

## Data Flow Through the Project

Understanding how data moves through the system:

```
1. Data Collection
   └─→ Store in data_files/raw/<experiment>/

2. Automated Processing (bash_scripts/)
   └─→ Validate raw data
   └─→ Process using core/src/ scripts
   └─→ Generate data_files/processed/<experiment>/

3. Analysis & Visualization
   ├─→ Automated: bash_scripts/ → output/plots/
   └─→ Interactive: notebooks/ → output/plots|tables/

4. Results & Reporting
   └─→ Use processed data and outputs for publications
```

---

## Design Principles

The project structure follows these key principles:

1. **Separation of Concerns**: Raw data, processing code, and outputs
   are clearly separated

2. **Reproducibility**: Raw data is immutable; processed data can be
   regenerated

3. **Modularity**: Reusable components in `core/library/` separate from
   domain-specific code

4. **Automation**: BASH scripts orchestrate workflows to minimize manual
   intervention

5. **Flexibility**: Notebooks provide interactive analysis while
   maintaining pipeline integrity

6. **Documentation**: Comprehensive docs ensure long-term
   maintainability

---

## File Naming Conventions

### Raw Data Files
Use descriptive names that capture experimental parameters:
- `<method>_<parameter>_<variation>.txt`
- Example: `KL_Brillouin_mu1p0_rho1p0_config0000200.txt`

### Processed Data Files
Match the experiment directory name:
- `<experiment_name>.csv`
- `<experiment_name>.h5`
- Example: `Chebyshev_several_config_varying_N.csv`

### Scripts
Use clear, action-oriented names:
- `process_*.py` for processing scripts
- `analyze_*.py` for analysis scripts
- `plot_*.py` for visualization scripts

---

## Maintenance Notes

### When Adding New Features

1. **New utility function?** → `core/library/`
2. **New analysis method?** → `core/src/`
3. **New workflow?** → `bash_scripts/workflows/`
4. **New test?** → `core/tests/`

### When Processing New Data

1. Create experiment directory in `data_files/raw/`
2. Place raw files there
3. Run appropriate BASH script from `bash_scripts/`
4. Processed files appear in `data_files/processed/`
5. Use notebooks for custom exploration

### Regular Tasks

- Run tests before committing: `pytest core/tests/`
- Update documentation when changing structure
- Archive old project plans but keep for reference
- Clean up output directories periodically

---
